{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80da3fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "import segmentation_models_pytorch as smp\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd6a0518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#constants\n",
    "labels = \"labels/\"\n",
    "images = \"images/\"\n",
    "channels = 3\n",
    "rate_learning = 1e-3\n",
    "epochs = 300\n",
    "bs = 10\n",
    "k_prop = 0.8\n",
    "wd = 1e-3\n",
    "h  = 256\n",
    "w = 192\n",
    "loss_list = []\n",
    "\n",
    "total_acc_train = np.zeros(epochs)\n",
    "total_acc_test = np.zeros(epochs)\n",
    "total_dice_train = np.zeros(epochs)\n",
    "total_dice_test = np.zeros(epochs)\n",
    "#cuda\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cac294e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create model\n",
    "def model_create():\n",
    "    accuracy = dice_score\n",
    "    model = smp.Unet(encoder_name='efficientnet-b1', in_channels=channels, classes=1, activation=None).to(device)\n",
    "    loss = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=rate_learning, weight_decay=wd)\n",
    "    return model, optimizer, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "107d3618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, transform=None, target_transform=None):\n",
    "        self.images  = os.listdir(images)\n",
    "        self.labels = os.listdir(labels)\n",
    "        \n",
    "        self.transform  = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(images, self.images[index])\n",
    "        mask_path = os.path.join(labels, self.images[index])\n",
    "        image = read_image(img_path)\n",
    "        label = read_image(mask_path, mode=ImageReadMode.GRAY)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            label = torch.cat([label], dim=0)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e726675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#splitting\n",
    "def splitting(data):\n",
    "    length = data.__len__()\n",
    "    test_length = length - int(k_prop * length)\n",
    "    train_length = int(k_prop * length)\n",
    "\n",
    "    (train, test) = torch.utils.data.random_split(data, [train_length, test_length])\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# dataload function\n",
    "def dataload(train, test):\n",
    "    train_loader = DataLoader(train, batch_size=bs, shuffle=True)\n",
    "    test_loader = DataLoader(test, batch_size=bs, shuffle=True )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c87fc4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pixel accuracy\n",
    "def dice_score(preds, labels):\n",
    "    dice_score  = 0\n",
    "    preds = torch.sigmoid(preds)\n",
    "    preds = (preds > 0.5).float()\n",
    "    dice_score += (2 * (preds * labels).sum())/((preds + labels).sum() + 1e-7)\n",
    "    return dice_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad104bad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_model(model, optim, trainloader, testloader, loss_func, accuracy, cnt):\n",
    "    train_acc = []\n",
    "    train_dice = []\n",
    "    \n",
    "    test_dice = []\n",
    "    test_acc = []\n",
    "    \n",
    "    for data, labels in trainloader:\n",
    "        model.train()\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optim.zero_grad()\n",
    "        target = model(data)\n",
    "\n",
    "        dice = accuracy(target, labels)\n",
    "        \n",
    "        loss = loss_func(target, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        train_dice.append(dice.cpu())\n",
    "        \n",
    "    \n",
    "    train_dice = (np.array(train_dice)).mean()\n",
    "    \n",
    "    transformer = transforms.ToPILImage()\n",
    "    \n",
    "    sigma = nn.Sigmoid()\n",
    "    \n",
    "    for data, labels in testloader:\n",
    "        model.eval()\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        target = model(data)\n",
    "    \n",
    "        target = sigma(target)\n",
    "        if (cnt%100==0): \n",
    "            for i in range((np.array(target.shape))[0]):\n",
    "                temp_target = transformer(target[i])\n",
    "                temp_labels = transformer(labels[i])\n",
    "                temp_target.show()\n",
    "                temp_labels.show()\n",
    "    \n",
    "    cnt += 1 \n",
    "\n",
    "    return model, train_dice, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55f944fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "    transforms.ConvertImageDtype(dtype=torch.float32),\n",
    "    transforms.Resize([h, w]),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.25]),\n",
    "    transforms.accimage\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "    transforms.ConvertImageDtype(dtype=torch.float32),\n",
    "    transforms.Resize([h, w]),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.25])\n",
    "    ])\n",
    "    \n",
    "    nails = SegmentationDataset(train_transform, test_transform)\n",
    "    \n",
    "    (train, test) = splitting(nails)\n",
    "    \n",
    "    train_loader, test_loader = dataload(train, test)\n",
    "\n",
    "    model, optimizer, loss, dice = model_create()\n",
    "\n",
    "    total_acc = np.zeros(epochs)\n",
    "    class_total_acc = np.zeros(epochs)\n",
    "    cnt = 0\n",
    "    for i in range(epochs):\n",
    "        model, train_dice, cnt = run_model(model, optimizer, train_loader, test_loader, loss, dice, cnt)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f15e585e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 56,  55,  38,  ..., 138, 135, 132],\n",
      "         [ 46,  35,  24,  ..., 137, 139, 141],\n",
      "         [ 20,  19,  37,  ..., 134, 136, 138],\n",
      "         ...,\n",
      "         [ 20,   0,  21,  ...,   4,  17,  27],\n",
      "         [ 46,   6,  10,  ...,  17,  31,  32],\n",
      "         [ 69,  19,   8,  ...,  26,  34,  28]],\n",
      "\n",
      "        [[ 76,  75,  58,  ..., 145, 142, 139],\n",
      "         [ 66,  55,  44,  ..., 144, 146, 148],\n",
      "         [ 40,  39,  57,  ..., 141, 143, 145],\n",
      "         ...,\n",
      "         [ 41,  21,  45,  ...,  21,  34,  44],\n",
      "         [ 66,  26,  30,  ...,  34,  48,  49],\n",
      "         [ 86,  36,  25,  ...,  45,  52,  46]],\n",
      "\n",
      "        [[101, 100,  82,  ..., 153, 150, 147],\n",
      "         [ 91,  80,  69,  ..., 152, 154, 156],\n",
      "         [ 65,  64,  81,  ..., 149, 151, 153],\n",
      "         ...,\n",
      "         [ 84,  66,  93,  ...,  31,  44,  54],\n",
      "         [ 93,  63,  89,  ...,  42,  58,  59],\n",
      "         [ 96,  66,  94,  ...,  52,  62,  56]]], dtype=torch.uint8)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[63], line 28\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 28\u001b[0m     model, train_dice, cnt \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[62], line 8\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(model, optim, trainloader, testloader, loss_func, accuracy, cnt)\u001b[0m\n\u001b[1;32m      5\u001b[0m test_dice \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, labels \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     10\u001b[0m     data, labels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/workspace/dolganin/detect_gesture/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/workspace/dolganin/detect_gesture/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/workspace/dolganin/detect_gesture/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/workspace/dolganin/detect_gesture/lib/python3.10/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/workspace/dolganin/detect_gesture/lib/python3.10/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[59], line 20\u001b[0m, in \u001b[0;36mSegmentationDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(image)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 20\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[1;32m     22\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(label)\n",
      "File \u001b[0;32m/workspace/dolganin/detect_gesture/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862deb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64302478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detect_gesture",
   "language": "python",
   "name": "detect_gesture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
