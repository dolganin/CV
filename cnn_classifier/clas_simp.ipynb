{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46185d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms, models\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed118a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# начнем нашу работу с данными:\n",
    "1. Проверим распредееление данных по персонажам, если оно слишком неравномерное -> придется делать аугментацию\n",
    "2. \"Трансформируем\" наши данные в тензоры \n",
    "3. Нормализуем их -> нам не надо беспокоиться о выбросах "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aaf05c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"константы\" с которыми будут работать наши классы и функции\n",
    "data_path = \"simpsons_dataset\"\n",
    "bs = 128\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f187eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#узнаем, сколько у нас вообще изображений и посмотри на распределение \n",
    "\n",
    "persons= {}\n",
    "\n",
    "# Проходим по всем папкам в директории\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    folder_name = os.path.basename(root)\n",
    "    if folder_name  == \"simpsons_dataset\":\n",
    "        continue\n",
    "    \n",
    "    file_count = len(files)\n",
    "    persons[folder_name] = file_count\n",
    "\n",
    "cnt = 0\n",
    "for name in persons.keys():\n",
    "    cnt += persons[name]\n",
    "    \n",
    "print(\"Всего изоражений:\", cnt)    \n",
    "persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d616af6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(persons.values(), persons.keys())\n",
    "df.columns = ['Количество изображений']\n",
    "plot = df.plot.bar(\n",
    "    figsize=(20, 5), fontsize=8, color='limegreen', \n",
    "    edgecolor='darkblue', linewidth=1, title='Количество изображений по персонажам\\n')\n",
    "plot.title.set_size(18)\n",
    "plot.legend(loc=1, fontsize=10)\n",
    "plot.set(ylabel=None, xlabel=None)\n",
    "plot.grid()\n",
    "plt.xticks(rotation=65)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366947a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Видим, что у нас сильный дизбаланс классов: где-то изображений 2000, а где-то и 10 нет => придется делать аугментацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ffb0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(persons.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea07881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_class_counts(dataset):\n",
    "    class_counts = torch.zeros(len(dataset.classes), 1)\n",
    "    for _, label in dataset:\n",
    "        class_counts[label] += 1\n",
    "    print(class_counts)\n",
    "    return class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b969bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#по сути этот шаг(аугментация) включает в себя и трансформацию в тензоры, и нормализацию данных)\n",
    "#создадим класс, который подготовит наши данные для обучения и теста\n",
    "class Dataset_prep():\n",
    "    \"\"\"\n",
    "    класс, который подготовит наш датасет к обучению: \n",
    "    он разобьет наши данные на train и test, переведет изображения в тезоры и нормализует их\n",
    "    \"\"\"   \n",
    "    #инициализируем наш класс, т.е. передадим ему данные, с которыми он будет рабоать  \n",
    "    def __init__(self,path):\n",
    "        super().__init__()\n",
    "        self.dataset = ImageFolder(path, transform=self.create_transformer())\n",
    "        \n",
    "    #создаем трансформер, который переводит изображения в тензоры и нормализует их    \n",
    "    def create_transformer(self):\n",
    "        transformer = transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.ConvertImageDtype(dtype=torch.float32),\n",
    "                                  transforms.Resize([100, 100]),\n",
    "                                  transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                  transforms.RandomRotation(15),\n",
    "                                  transforms.Normalize(mean=[0.5], std=[0.25])]\n",
    "                                )\n",
    "        return transformer\n",
    "    \n",
    "    def augment_data(self, dataset):\n",
    "        augmented_dataset = []\n",
    "        class_counts = get_class_counts(dataset)\n",
    "        for image, label in dataset:\n",
    "            while class_counts[label] < 500:\n",
    "                # копируем оригинальное изображение\n",
    "                augmented_dataset.append((image, label))\n",
    "                if random.randint(0,1) == 0:\n",
    "                    # проводим горизонтальное отражение\n",
    "                    augmented_dataset.append((transforms.functional.hflip(image), label))\n",
    "                elif random.randint(0,1) == 0:\n",
    "                    augmented_dataset.append((transforms.functional.vflip(image), label))\n",
    "                else:\n",
    "                    # проводим повороты на случайный угол\n",
    "                    angle = random.randint(-40,40)\n",
    "                    augmented_dataset.append((transforms.functional.rotate(image, angle), label))\n",
    "                    \n",
    "        return augmented_dataset\n",
    "    \n",
    "    #разбиваем наши данные на traint,val и test\n",
    "    def split_and_transform(self):\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        train, test = train_test_split(self.dataset, train_size=0.8, random_state=42, shuffle = True, stratify=self.dataset.classes)\n",
    "        \n",
    "        train, val = train_test_split(train, train_size=0.8, random_state=42, shuffle = True, stratify=train.classes)\n",
    "        \n",
    "        augmented_train = self.augment_data(train)\n",
    "        return augmented_train,val, test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99806b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ImageFolder(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22ae8def",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [21046, 42]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m preporations \u001b[38;5;241m=\u001b[39m Dataset_prep(data_path)\n\u001b[0;32m----> 2\u001b[0m train,val,test \u001b[38;5;241m=\u001b[39m \u001b[43mpreporations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_and_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train, batch_size\u001b[38;5;241m=\u001b[39mbs, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(val, batch_size\u001b[38;5;241m=\u001b[39mbs, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[47], line 47\u001b[0m, in \u001b[0;36mDataset_prep.split_and_transform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_and_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m---> 47\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     train, val \u001b[38;5;241m=\u001b[39m train_test_split(train, train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, stratify\u001b[38;5;241m=\u001b[39mtrain\u001b[38;5;241m.\u001b[39mclasses)\n\u001b[1;32m     51\u001b[0m     augmented_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugment_data(train)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:2583\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2579\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2581\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2583\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   2586\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2587\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:1688\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \n\u001b[1;32m   1661\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;124;03m    to an integer.\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1688\u001b[0m     X, y, groups \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1689\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[1;32m   1690\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 443\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [21046, 42]"
     ]
    }
   ],
   "source": [
    "preporations = Dataset_prep(data_path)\n",
    "train,val,test = preporations.split_and_transform()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=bs, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=bs, shuffle=True)\n",
    "print(\"Теперь train содержит: \",len(train),\"val: \",len(val),\" и test:\",len(test),\".В общем мы имеем \",  len(train)+len(val)+len(test),\"изображений\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb382382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
